{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitignore', '.nojekyll', 'apex', 'csrc', 'docs', 'examples', 'LICENSE', 'README.md', 'setup.py', 'tests']\n"
     ]
    }
   ],
   "source": [
    "# Version 2 + Bug fix - thanks to @chinhuic\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../01_toxic_proj/input/NVIDIA-apex-39e153a\"))\n",
    "#print(os.listdir(\"C:\\\\Users\\\\Bo\\\\Desktop\\\\01_toxic_proj\\\\input\\\\NVIDIA-apex-39e153a\"))\n",
    "#print(os.listdir(\"../input/NVIDIA-apex-39e153a\"))\n",
    "#print(os.listdir(\"../input/glove-global-vectors-for-word-representation\"))\n",
    "#print(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\n",
    "#print(os.listdir(\"../input/fasttext-crawl-300d-2m\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Nvidia Apex\n",
    "! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../01_toxic_proj/input/NVIDIA-apex-39e153a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "#from apex import amp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 220\n",
    "SEED = 1234\n",
    "EPOCHS = 1\n",
    "Data_dir=\"../01_toxic_proj/input/jigsaw-unintended-bias-in-toxicity-classification\"\n",
    "Input_dir = \"../01_toxic_proj/input\"\n",
    "WORK_DIR = \"../01_toxic_proj/working/\"\n",
    "num_to_load=1000000                         #Train size to match time limit\n",
    "valid_size= 100000                          #Validation Size\n",
    "TOXICITY_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Bart Pytorch repo to the PATH\n",
    "# using files from: https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "package_dir_a = \"../01_toxic_proj/input/pytorch-pretrained-BERT\"\n",
    "sys.path.insert(0, package_dir_a)\n",
    "\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from C:\\Users\\Bo\\Desktop\\01_toxic_proj\\input\\pytorch-pretrained-BERT\\uncased_L-24_H-1024_A-16\\bert_model.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 1024]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 1024]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_12/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_12/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/key/bias with shape [1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_13/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_13/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_13/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_14/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_14/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_15/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_15/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_16/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_16/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_17/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_17/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_18/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_18/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/output/dense/kernel with shape [1024, 1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_19/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_19/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_19/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_20/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_20/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_21/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_21/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_22/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_22/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_23/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_23/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [1024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [1024, 1024]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [4096]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [1024, 4096]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [1024]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [4096, 1024]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [1024]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [1024]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [1024, 1024]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 1024]\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_12', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_13', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_14', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_15', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_16', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'key', 'kernel']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_17', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_18', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_19', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_20', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'intermediate', 'dense', 'kernel']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_21', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_22', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_23', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to ../01_toxic_proj/working/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../01_toxic_proj/working/bert_config.json'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate model from tensorflow to pytorch\n",
    "BERT_MODEL_PATH = '../01_toxic_proj/input/pytorch-pretrained-BERT/uncased_L-24_H-1024_A-16/'\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "BERT_MODEL_PATH + 'bert_config.json',\n",
    "WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_config.json', 'pytorch_model.bin']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../01_toxic_proj/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Bert configuration file\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "bert_config = BertConfig('../01_toxic_proj/input/pytorch-pretrained-BERT/uncased_L-24_H-1024_A-16/'+'bert_config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lines to BERT format\n",
    "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
    "from ipywidgets import IntProgress\n",
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    max_seq_length -=2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    for text in tqdm_notebook(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
    "        all_tokens.append(one_token)\n",
    "    print(longer)\n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_PATH = '../01_toxic_proj/input/pytorch-pretrained-BERT/uncased_L-24_H-1024_A-16/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.7.1\n",
      "asn1crypto==0.24.0\n",
      "astor==0.7.1\n",
      "attrs==19.1.0\n",
      "backcall==0.1.0\n",
      "bleach==3.1.0\n",
      "boto3==1.9.162\n",
      "botocore==1.12.163\n",
      "certifi==2019.3.9\n",
      "cffi==1.12.3\n",
      "chardet==3.0.4\n",
      "colorama==0.4.1\n",
      "cryptography==2.7\n",
      "cycler==0.10.0\n",
      "decorator==4.4.0\n",
      "defusedxml==0.6.0\n",
      "docutils==0.14\n",
      "entrypoints==0.3\n",
      "gast==0.2.2\n",
      "grpcio==1.16.1\n",
      "h5py==2.9.0\n",
      "idna==2.8\n",
      "ipykernel==5.1.1\n",
      "ipython==7.5.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.2\n",
      "jedi==0.13.3\n",
      "Jinja2==2.10.1\n",
      "jmespath==0.9.4\n",
      "joblib==0.13.2\n",
      "jsonschema==3.0.1\n",
      "jupyter-client==5.2.4\n",
      "jupyter-core==4.4.0\n",
      "Keras-Applications==1.0.8\n",
      "Keras-Preprocessing==1.1.0\n",
      "kiwisolver==1.1.0\n",
      "Markdown==3.1.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.1.0\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.12\n",
      "mkl-random==1.0.2\n",
      "mkl-service==2.0.2\n",
      "mock==3.0.5\n",
      "nbconvert==5.5.0\n",
      "nbformat==4.4.0\n",
      "nltk==3.4.1\n",
      "notebook==5.7.8\n",
      "numpy==1.16.4\n",
      "pandas==0.24.2\n",
      "pandocfilters==1.4.2\n",
      "parso==0.4.0\n",
      "patsy==0.5.1\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.0.0\n",
      "prometheus-client==0.6.0\n",
      "prompt-toolkit==2.0.9\n",
      "protobuf==3.8.0\n",
      "pycparser==2.19\n",
      "Pygments==2.4.2\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.4.0\n",
      "pyreadline==2.1\n",
      "pyrsistent==0.14.11\n",
      "PySocks==1.7.0\n",
      "python-dateutil==2.8.0\n",
      "pytz==2019.1\n",
      "pywinpty==0.5.5\n",
      "pyzmq==18.0.0\n",
      "regex==2019.6.5\n",
      "requests==2.22.0\n",
      "s3transfer==0.2.0\n",
      "scikit-learn==0.21.2\n",
      "scipy==1.2.1\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "six==1.12.0\n",
      "statsmodels==0.9.0\n",
      "tensorboard==1.13.1\n",
      "tensorflow==1.13.1\n",
      "tensorflow-estimator==1.13.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.2\n",
      "testpath==0.4.2\n",
      "torch==1.1.0\n",
      "torchvision==0.3.0\n",
      "tornado==6.0.2\n",
      "tqdm==4.32.1\n",
      "traitlets==4.3.2\n",
      "urllib3==1.24.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.15.4\n",
      "widgetsnbextension==3.4.2\n",
      "win-inet-pton==1.1.0\n",
      "wincertstore==0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\utils\\_process_win32.py:131: ResourceWarning: unclosed file <_io.BufferedWriter name=4>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "G:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\utils\\_process_win32.py:131: ResourceWarning: unclosed file <_io.BufferedReader name=5>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "G:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\utils\\_process_win32.py:131: ResourceWarning: unclosed file <_io.BufferedReader name=6>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The conda.compat module is deprecated and will be removed in a future release.\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.11\n",
      "  latest version: 4.6.14\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1100000 records\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of               id    target                                       comment_text  \\\n",
       "458232    806064  0.000000  It's difficult for many old people to keep up ...   \n",
       "272766    576402  0.166667  She recognized that her tiny-handed husband is...   \n",
       "339129    658508  0.000000  HPHY76,\\nGood for you for thinking out loud, w...   \n",
       "773565   5066714  0.500000  And I bet that in the day you expected your Je...   \n",
       "476233    828147  0.000000  Kennedy will add a much needed and scientifica...   \n",
       "317668    631532  0.300000  Yeah, because it is far more important to pres...   \n",
       "839167   5147509  0.000000          Housing inmates is a profitable business.   \n",
       "913935   5237995  0.000000                                           Go Sens!   \n",
       "904703   5226678  0.200000  Lol...sounds like someone IS brainwashed but I...   \n",
       "1579488  6054559  0.000000  Tax relief should include commercial real esta...   \n",
       "963680   5297004  0.100000  It's so simple to trash Trump the job should b...   \n",
       "401755    734360  0.000000                    8a welfare native corporations.   \n",
       "1001442  5342229  0.000000  This whole debacle is the worst sort of joke. ...   \n",
       "1014498  5358061  0.700000  Not in the last fifty years! Its all been abou...   \n",
       "1729558  6242913  0.000000  Opposite ? How does it feel to know that we kn...   \n",
       "246884    545725  0.000000  Perhaps you should address your appeal to him....   \n",
       "162713    440814  0.111111  Yes, it does and please note that those evokin...   \n",
       "47271     299774  0.400000  The person with the abusive husband needs to g...   \n",
       "1170549  5546499  0.200000  President man-baby and Mr. Exxon Valdez negoti...   \n",
       "1272446  5669604  0.500000  Well, I guess it didn't matter so much when Ob...   \n",
       "1563977  6035414  0.166667  The oligarchs and their toadies have the Presi...   \n",
       "373844    700623  0.000000  。。。and return the annexed Hawaii to the rightf...   \n",
       "33722     283124  0.000000  I agree. I know Mr. Folger personally and he i...   \n",
       "713438   4995465  0.000000  \"Those that hang around downtown all day\" are ...   \n",
       "226337    519659  0.166667  Crying about being moderated is the stance of ...   \n",
       "677165   1069903  0.000000  For the life of me I don't see how the Republi...   \n",
       "1008419  5350527  0.000000  But my first response is beside the point: I a...   \n",
       "4111      246222  0.000000  I think we should be careful about making accu...   \n",
       "578789    950419  0.600000  The best thing since Kennedy's assassination? ...   \n",
       "729212   5014078  0.200000  Wilders gained 5 seats from the last election ...   \n",
       "...          ...       ...                                                ...   \n",
       "1291882  5693574  0.000000  Because contrary to his hype, Trump couldn't f...   \n",
       "469471    819937  0.000000  Some of us having been here long enough, get t...   \n",
       "768746   5060949  0.600000  Well, you sound pretty dumb.  One space?  How ...   \n",
       "975238   5310941  0.000000  What is unsafe about the stadium is the stadiu...   \n",
       "1011223  5354209  0.000000  \"Estimated injury costs per mile bicycled fell...   \n",
       "1263718  5659463  0.000000  There is no end to the scheming anc members wi...   \n",
       "1312771  5719567  0.000000  No that would be the Premier of BC and his Gre...   \n",
       "30329     278964  0.166667  If the Republican Party can't decide on a nomi...   \n",
       "1173007  5549330  0.000000                         Who let the monkeys out???   \n",
       "1027775  5373822  0.100000  Thanks for this article Dave. I had no clue it...   \n",
       "1041921  5390841  0.000000  King George was not term limited to eight year...   \n",
       "1315044  5722482  0.600000  Great!  The Donald will have his Russian pals ...   \n",
       "1563636  6035006  0.000000  Burnsie: seriously?  This has nothing to do wi...   \n",
       "550549    916603  0.400000  Why is it a crime?\\nIt's a TEMPORARY SUSPENSIO...   \n",
       "388512    718153  0.166667  There's also the corollary ... Trump's 'suppor...   \n",
       "1789197  6314294  0.200000  It is there to verify identity when requesting...   \n",
       "407994    741991  0.166667  The car is an abomination and illustrates ever...   \n",
       "1442897  5883443  0.200000  Tip to House11 and the 6 others: Jewishophobia...   \n",
       "118461    386956  0.500000  She should have gone to jail with her crooked ...   \n",
       "1305679  5710502  0.000000  Obviously no president can sign a Bill of this...   \n",
       "294272    602420  0.000000  Regarding:\"...thus the very raison d'être of t...   \n",
       "1068963  5422888  0.000000  Sorry, Putie, but your top asset in the United...   \n",
       "1623855  6111009  0.000000  Did Charles make that claim? \\nI think you're ...   \n",
       "681160   1074873  0.000000  Watched one episode. Arnold stinks in the CEO ...   \n",
       "86380     348087  0.400000  A Trans man is a man that wishes he was a woma...   \n",
       "33428     282773  0.000000  \"30 times the national average\"? I seriously d...   \n",
       "1229137  5616877  0.000000  If you looked at the pictures the unit wasn't ...   \n",
       "1801934  6329908  0.000000  The pipeline would subsidize the rail line.\\nW...   \n",
       "410919    745971  0.000000  Detox is the solution and it is realistic. We ...   \n",
       "1087506  5445604  0.000000  WOW! Nothing sweet nor reasonable about this c...   \n",
       "\n",
       "         severe_toxicity   obscene  identity_attack    insult  threat  asian  \\\n",
       "458232               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "272766               0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "339129               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "773565               0.0  0.000000              0.4  0.100000     0.0    0.0   \n",
       "476233               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "317668               0.0  0.100000              0.2  0.100000     0.0    NaN   \n",
       "839167               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "913935               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "904703               0.0  0.000000              0.0  0.200000     0.0    NaN   \n",
       "1579488              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "963680               0.0  0.000000              0.0  0.000000     0.1    NaN   \n",
       "401755               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1001442              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1014498              0.1  0.000000              0.6  0.200000     0.0    0.0   \n",
       "1729558              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "246884               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "162713               0.0  0.111111              0.0  0.000000     0.0    NaN   \n",
       "47271                0.1  0.100000              0.0  0.400000     0.0    NaN   \n",
       "1170549              0.0  0.000000              0.0  0.200000     0.0    0.0   \n",
       "1272446              0.0  0.100000              0.1  0.400000     0.0    0.0   \n",
       "1563977              0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "373844               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "33722                0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "713438               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "226337               0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "677165               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1008419              0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "4111                 0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "578789               0.0  0.000000              0.4  0.400000     0.0    0.0   \n",
       "729212               0.0  0.000000              0.0  0.200000     0.0    0.0   \n",
       "...                  ...       ...              ...       ...     ...    ...   \n",
       "1291882              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "469471               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "768746               0.1  0.100000              0.1  0.700000     0.0    NaN   \n",
       "975238               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1011223              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1263718              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1312771              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "30329                0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1173007              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1027775              0.0  0.100000              0.0  0.000000     0.0    NaN   \n",
       "1041921              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1315044              0.0  0.000000              0.1  0.600000     0.0    NaN   \n",
       "1563636              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "550549               0.0  0.000000              0.2  0.300000     0.0    NaN   \n",
       "388512               0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "1789197              0.0  0.000000              0.2  0.000000     0.0    0.0   \n",
       "407994               0.0  0.000000              0.0  0.166667     0.0    NaN   \n",
       "1442897              0.2  0.000000              0.2  0.200000     0.0    0.0   \n",
       "118461               0.0  0.000000              0.0  0.500000     0.0    NaN   \n",
       "1305679              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "294272               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1068963              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1623855              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "681160               0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "86380                0.0  0.000000              0.4  0.200000     0.0    0.0   \n",
       "33428                0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1229137              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1801934              0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "410919               0.0  0.000000              0.0  0.000000     0.0    NaN   \n",
       "1087506              0.0  0.000000              0.0  0.000000     0.0    0.0   \n",
       "\n",
       "         atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "458232       NaN  ...      160581  approved      0    0    0      1         0   \n",
       "272766       NaN  ...      150555  approved      0    0    0      2         0   \n",
       "339129       NaN  ...      154368  approved      0    0    0      0         0   \n",
       "773565       0.0  ...      322772  approved      0    0    0      4         2   \n",
       "476233       NaN  ...      161073  approved      0    0    1      0         0   \n",
       "317668       NaN  ...      152691  approved      0    0    0      0         0   \n",
       "839167       NaN  ...      327622  approved      0    0    0      0         0   \n",
       "913935       NaN  ...      333339  approved      0    0    0      0         0   \n",
       "904703       NaN  ...      332690  approved      1    0    1      6         2   \n",
       "1579488      NaN  ...      383658  rejected      0    0    0      0         0   \n",
       "963680       NaN  ...      337065  approved      0    0    0      2         3   \n",
       "401755       NaN  ...      157569  approved      0    0    0      2         1   \n",
       "1001442      NaN  ...      339685  rejected      0    0    0      0         0   \n",
       "1014498      0.0  ...      340648  approved      0    0    1      0         0   \n",
       "1729558      NaN  ...      393318  approved      0    0    0      1         0   \n",
       "246884       NaN  ...      149232  approved      0    0    0      4         0   \n",
       "162713       NaN  ...      144158  approved      0    0    0      0         0   \n",
       "47271        NaN  ...       98862  approved      0    0    0      0         0   \n",
       "1170549      0.0  ...      352764  approved      2    0    0      5         2   \n",
       "1272446      0.0  ...      359868  approved      0    0    0      5         0   \n",
       "1563977      NaN  ...      382640  approved      0    0    0     11         3   \n",
       "373844       NaN  ...      156038  approved      0    0    0      1         0   \n",
       "33722        0.0  ...       99079  approved      0    0    0      1         0   \n",
       "713438       NaN  ...      318773  approved      0    0    0      1         0   \n",
       "226337       NaN  ...      148115  approved      0    0    0      5         0   \n",
       "677165       NaN  ...      316277  approved      0    0    0      2         1   \n",
       "1008419      0.0  ...      340172  approved      0    0    0      0         0   \n",
       "4111         0.0  ...       41835  approved      0    0    0      1         0   \n",
       "578789       0.0  ...      166201  approved      0    0    0      2         0   \n",
       "729212       0.0  ...      319919  rejected      0    0    0      0         0   \n",
       "...          ...  ...         ...       ...    ...  ...  ...    ...       ...   \n",
       "1291882      NaN  ...      361386  approved      0    0    0      1         0   \n",
       "469471       NaN  ...      160998  approved      0    0    0      1         0   \n",
       "768746       NaN  ...      322436  approved      1    0    0      5         0   \n",
       "975238       NaN  ...      337893  approved      0    0    0      0         0   \n",
       "1011223      NaN  ...      340539  approved      0    0    0      0         0   \n",
       "1263718      NaN  ...      359277  approved      2    0    0      2         0   \n",
       "1312771      NaN  ...      363122  rejected      0    0    0      0         0   \n",
       "30329        NaN  ...       58977  approved      0    0    0      0         0   \n",
       "1173007      NaN  ...      352815  approved      3    0    0      1         1   \n",
       "1027775      NaN  ...      341517  approved      0    0    0      9         0   \n",
       "1041921      NaN  ...      342420  approved      1    0    0      2         0   \n",
       "1315044      NaN  ...      363465  approved      2    2    0      7         3   \n",
       "1563636      NaN  ...      382128  approved      0    0    0      2         0   \n",
       "550549       NaN  ...      164850  approved      0    0    0     18         0   \n",
       "388512       NaN  ...      156549  approved      0    0    0      0         1   \n",
       "1789197      0.0  ...      398062  rejected      0    0    0      0         0   \n",
       "407994       NaN  ...      157993  approved      0    2    2      0        10   \n",
       "1442897      0.0  ...      373167  approved      2    0    0      0         3   \n",
       "118461       NaN  ...      141273  approved      0    0    0      2         0   \n",
       "1305679      NaN  ...      362647  approved      0    0    0      1         0   \n",
       "294272       NaN  ...      151610  approved      0    0    0      0         0   \n",
       "1068963      NaN  ...      344698  approved      0    0    0      1         0   \n",
       "1623855      NaN  ...      386695  approved      0    0    0      0         0   \n",
       "681160       0.0  ...      316686  approved      2    0    0      0         0   \n",
       "86380        0.0  ...      138625  approved      0    0    0      1         0   \n",
       "33428        NaN  ...       59330  approved      0    0    0      0         0   \n",
       "1229137      NaN  ...      356543  approved      0    0    0      0         0   \n",
       "1801934      NaN  ...      399168  approved      1    0    0      2         0   \n",
       "410919       NaN  ...      158100  approved      0    0    0      1         2   \n",
       "1087506      0.0  ...      346143  approved      0    0    0      2         0   \n",
       "\n",
       "         sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "458232               0.0                         0                         4  \n",
       "272766               0.0                         0                         6  \n",
       "339129               0.0                         0                         4  \n",
       "773565               0.0                         6                        10  \n",
       "476233               0.0                         0                         4  \n",
       "317668               0.1                         0                        10  \n",
       "839167               0.0                         0                         4  \n",
       "913935               0.0                         0                         4  \n",
       "904703               0.0                         0                        10  \n",
       "1579488              0.0                         0                         4  \n",
       "963680               0.0                         0                        10  \n",
       "401755               0.0                         0                         4  \n",
       "1001442              0.0                         0                         4  \n",
       "1014498              0.0                        10                        10  \n",
       "1729558              0.0                         0                         4  \n",
       "246884               0.0                         0                         4  \n",
       "162713               0.0                         0                         9  \n",
       "47271                0.1                         0                        10  \n",
       "1170549              0.0                        10                         5  \n",
       "1272446              0.0                         6                        10  \n",
       "1563977              0.0                         0                         6  \n",
       "373844               0.0                         0                         4  \n",
       "33722                0.0                        10                         4  \n",
       "713438               0.0                         0                         4  \n",
       "226337               0.0                         0                         6  \n",
       "677165               0.0                         0                         4  \n",
       "1008419              0.0                         4                         4  \n",
       "4111                 0.0                        10                         4  \n",
       "578789               0.0                         5                        10  \n",
       "729212               0.0                        10                         5  \n",
       "...                  ...                       ...                       ...  \n",
       "1291882              0.0                         0                         4  \n",
       "469471               0.0                         0                         5  \n",
       "768746               0.0                         0                        10  \n",
       "975238               0.0                         0                         4  \n",
       "1011223              0.0                         0                         4  \n",
       "1263718              0.0                         0                         4  \n",
       "1312771              0.0                         0                         4  \n",
       "30329                0.0                         0                         6  \n",
       "1173007              0.0                         0                         4  \n",
       "1027775              0.1                         0                        10  \n",
       "1041921              0.0                         0                         4  \n",
       "1315044              0.0                         0                        10  \n",
       "1563636              0.0                         0                         4  \n",
       "550549               0.0                         0                        10  \n",
       "388512               0.0                         0                         6  \n",
       "1789197              0.0                         5                         5  \n",
       "407994               0.0                         0                         6  \n",
       "1442897              0.0                        10                         5  \n",
       "118461               0.0                         0                        10  \n",
       "1305679              0.0                         0                         4  \n",
       "294272               0.0                         0                         4  \n",
       "1068963              0.0                         0                         4  \n",
       "1623855              0.0                         0                         4  \n",
       "681160               0.0                         4                         4  \n",
       "86380                0.0                         4                        10  \n",
       "33428                0.0                         0                         4  \n",
       "1229137              0.0                         0                         4  \n",
       "1801934              0.0                         0                         4  \n",
       "410919               0.0                         0                         4  \n",
       "1087506              0.0                        10                         4  \n",
       "\n",
       "[1100000 rows x 45 columns]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all comment_text values are strings\n",
    "train_df['comment_text'] = train_df['comment_text'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIntProgress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IntProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-10381dda34ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comment_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DUMMY_VALUE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-7ea4b10d3b04>\u001b[0m in \u001b[0;36mconvert_lines\u001b[1;34m(example, max_seq_length, tokenizer)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlonger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtokens_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"See tqdm._tqdm_notebook.tqdm_notebook for full documentation\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tqdm_notebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         self.sp = self.status_printer(\n\u001b[1;32m--> 214\u001b[1;33m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# trick to place description before the bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# #187 #451 #558\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             raise ImportError(\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[1;34m\"IntProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "sequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.fillna(0)\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "y_columns=['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1100000 records\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIntProgress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IntProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-ecd0eb48bd67>\u001b[0m in \u001b[0;36mconvert_lines\u001b[1;34m(example, max_seq_length, tokenizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mall_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlonger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtokens_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;34m\"\"\"See tqdm._tqdm_notebook.tqdm_notebook for full documentation\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tqdm_notebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         self.sp = self.status_printer(\n\u001b[1;32m--> 214\u001b[1;33m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# trick to place description before the bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# #187 #451 #558\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             raise ImportError(\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[1;34m\"IntProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n",
    "train_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).sample(num_to_load+valid_size,random_state=SEED)\n",
    "print('loaded %d records' % len(train_df))\n",
    "\n",
    "# Make sure all comment_text values are strings\n",
    "train_df['comment_text'] = train_df['comment_text'].astype(str) \n",
    "\n",
    "sequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
    "train_df=train_df.fillna(0)\n",
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "y_columns=['target']\n",
    "\n",
    "train_df = train_df.drop(['comment_text'],axis=1)\n",
    "# convert target to 0,1\n",
    "train_df['target']=(train_df['target']>=0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7e66075300cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sequences' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "X = sequences[:num_to_load]                \n",
    "y = train_df[y_columns].values[:num_to_load]\n",
    "X_val = sequences[num_to_load:]                \n",
    "y_val = train_df[y_columns].values[num_to_load:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=train_df.tail(valid_size).copy()\n",
    "train_df=train_df.head(num_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_file = \"bert_pytorch.bin\"\n",
    "\n",
    "lr=2e-5\n",
    "batch_size = 32\n",
    "accumulation_steps=2\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"../working\",cache_dir=None,num_labels=len(y_columns))\n",
    "model.zero_grad()\n",
    "model = model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "train = train_dataset\n",
    "\n",
    "num_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=lr,\n",
    "                     warmup=0.05,\n",
    "                     t_total=num_train_optimization_steps)\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "model=model.train()\n",
    "\n",
    "tq = tqdm_notebook(range(EPOCHS))\n",
    "for epoch in tq:\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0.\n",
    "    lossf=None\n",
    "    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n",
    "    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n",
    "    for i,(x_batch, y_batch) in tk0:\n",
    "#        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            optimizer.zero_grad()\n",
    "        if lossf:\n",
    "            lossf = 0.98*lossf+0.02*loss.item()\n",
    "        else:\n",
    "            lossf = loss.item()\n",
    "        tk0.set_postfix(loss = lossf)\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), output_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation\n",
    "# The following 2 lines are not needed but show how to download the model for prediction\n",
    "model = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\n",
    "model.load_state_dict(torch.load(output_model_file ))\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.eval()\n",
    "valid_preds = np.zeros((len(X_val)))\n",
    "valid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n",
    "\n",
    "tk0 = tqdm_notebook(valid_loader)\n",
    "for i,(x_batch,)  in enumerate(tk0):\n",
    "    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
    "    valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From baseline kernel\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]>0.5\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "\n",
    "\n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]>0.5]\n",
    "    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n",
    "    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label]>0.5, examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n",
    "    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label]>0.5, examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = 'model1'\n",
    "test_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\n",
    "TOXICITY_COLUMN = 'target'\n",
    "bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n",
    "bias_metrics_df\n",
    "get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
