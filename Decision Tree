Entropy H(s)= -p1*log(p1) - ... -pn*log(pn)
pi is the percentage of data labeled as class ci

H(s) -> 0 when pi ~0 or 1, meaning the small entropy indicate a concentrated distribution for this class.
At each step, we calculate the H(s) and split the highest out, then repeat for the rest.

Cost functions:
Regression: sum(y - predictions)**2
Classification: G = sum(pk *(1-pl))

CART (classification and regression tree)
  classification is dealing with discrete features.
  regression is dealing with continuous features.

Advantages of CART:
1.Simple to understand, interpret, visualize.
2.Decision trees implicitly perform variable 
  screening or feature selection.
3.Can handle both numerical and categorical data. 
  Can also handle multi-output problems.
4.Decision trees require relatively little effort 
  from users for data preparation.
5.Nonlinear relationships between parameters do 
  not affect tree performance.

Disadvantages of CART
1.Decision-tree learners can create over-complex 
  trees that overfits, and this can be solved with 
  prunning; drop further brunches at the point of
  validation/test accuracy starts to drop while
  training accuracy is still pushing down.
2. Decision trees can be unstable because small 
  variations in the data might result in a completely 
  different tree being generated. This is called variance, 
  which needs to be lowered by methods like bagging and boosting.
3.Greedy algorithms cannot guarantee to return the 
  globally optimal decision tree. This can be mitigated 
  by training multiple trees, where the features and 
  samples are randomly sampled with replacement.
4.Decision tree learners create biased trees if some 
  classes dominate. It is therefore recommended 
  to balance the data set prior to fitting with the decision tree.
  
Optimize strateges:
  1.Us validation set besides test set to evaluating prunning node.
  2.Apply error estimation/significance testing(Chi-square test) to
    evaluate the prunning or expanding effect.
    Extension of Chi-square:
      The chi-square test can be used to determine the association 
      between categorical variables. It is based on the difference 
      between the expected frequencies (e) and the observed 
      frequencies (n) in one or more categories in the frequency 
      table. The chi-square distribution returns a probability for 
      the computed chi-square and the degree of freedom. 
      
      A probability of zero shows a complete dependency between two 
      categorical variables and a probability of one means that 
      two categorical variables are completely independent.
